"""
Helper module for publishing avatar video tracks from the backend agent.

This module provides utilities for integrating with Tavus or Beyond Presence
to generate and publish video tracks synchronized with the agent's audio output.

Usage:
    from avatar_video import publish_avatar_video
    
    # In your agent entrypoint
    await publish_avatar_video(ctx)
"""

import os
import asyncio
import logging
import numpy as np
from typing import Optional
from livekit import rtc
from livekit.agents import JobContext

logger = logging.getLogger(__name__)


async def publish_avatar_video(
    ctx: JobContext,
    provider: Optional[str] = None,
) -> Optional[rtc.LocalVideoTrack]:
    """
    Publish an avatar video track to the LiveKit room.
    
    This function creates a video track and publishes it to the room.
    The actual video frames would be generated by calling Tavus or Beyond Presence APIs.
    
    Args:
        ctx: JobContext from the agent entrypoint
        provider: Avatar provider ("tavus", "beyond-presence", or None for placeholder)
    
    Returns:
        LocalVideoTrack if successful, None otherwise
    """
    try:
        # Check if avatar is enabled
        enable_avatar = os.getenv("ENABLE_AVATAR_VIDEO", "false").lower() == "true"
        if not enable_avatar:
            logger.info("Avatar video disabled (set ENABLE_AVATAR_VIDEO=true to enable)")
            return None
        
        # Get provider from env or parameter
        if provider is None:
            provider = os.getenv("AVATAR_PROVIDER", "placeholder").lower()
        
        # Create video source (adjust resolution as needed)
        # Lower resolution = better performance, less lag
        # Default to 640x360 for better performance
        width = int(os.getenv("AVATAR_VIDEO_WIDTH", "640"))
        height = int(os.getenv("AVATAR_VIDEO_HEIGHT", "360"))
        fps = int(os.getenv("AVATAR_VIDEO_FPS", "15"))  # Lower FPS = less CPU usage
        
        logger.info(f"Creating avatar video track: {width}x{height} @ {fps}fps (provider: {provider})")
        
        video_source = rtc.VideoSource(width, height)
        video_track = rtc.LocalVideoTrack.create_video_track("avatar-video", video_source)
        
        # Publish the video track
        await ctx.room.local_participant.publish_track(video_track)
        logger.info(f"âœ… Published avatar video track ({width}x{height} @ {fps}fps)")
        
        # Start generating video frames in background
        if provider == "tavus":
            asyncio.create_task(_generate_tavus_frames(video_source, width, height, fps))
        elif provider == "beyond-presence":
            asyncio.create_task(_generate_beyond_presence_frames(video_source, width, height, fps))
        else:
            # Default: Generate placeholder/test pattern video
            asyncio.create_task(_generate_placeholder_frames(video_source, width, height, fps))
        
        return video_track
        
    except Exception as e:
        logger.error(f"Failed to publish avatar video: {e}")
        import traceback
        traceback.print_exc()
        return None


async def _generate_placeholder_frames(
    video_source: rtc.VideoSource,
    width: int,
    height: int,
    fps: int,
):
    """
    Generate placeholder video frames (test pattern).
    
    This creates a simple animated pattern that can be replaced with
    actual avatar video from Tavus or Beyond Presence.
    """
    logger.info(f"Generating placeholder avatar video frames ({width}x{height} @ {fps}fps)")
    frame_duration = 1.0 / fps
    frame_count = 0
    last_log_time = asyncio.get_event_loop().time()
    
    try:
        # Wait a bit for the track to be fully published
        await asyncio.sleep(0.5)
        
        while True:
            start_time = asyncio.get_event_loop().time()
            
            try:
                # Create frame (now optimized with vectorized operations)
                frame = _create_test_pattern_frame(width, height, frame_count)
                
                # Capture the frame (capture_frame is synchronous, not async)
                video_source.capture_frame(frame)
                
                frame_count += 1
                
                # Log every 5 seconds to reduce log spam
                current_time = asyncio.get_event_loop().time()
                if current_time - last_log_time >= 5.0:
                    logger.debug(f"Captured {frame_count} frames ({fps} fps target)")
                    last_log_time = current_time
                
                # Calculate how long frame generation took
                frame_time = asyncio.get_event_loop().time() - start_time
                
                # Sleep for remaining time to maintain frame rate
                sleep_time = max(0, frame_duration - frame_time)
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
                else:
                    # Frame generation took longer than frame duration - skip sleep
                    # This prevents lag from accumulating
                    logger.warning(f"Frame generation took {frame_time:.3f}s (target: {frame_duration:.3f}s) - skipping sleep")
                    
            except Exception as frame_error:
                logger.error(f"Error creating/capturing frame {frame_count}: {frame_error}")
                await asyncio.sleep(frame_duration)  # Continue even if one frame fails
            
    except asyncio.CancelledError:
        logger.info("Stopped generating placeholder frames")
    except Exception as e:
        logger.error(f"Error generating placeholder frames: {e}")
        import traceback
        traceback.print_exc()


def _create_test_pattern_frame(width: int, height: int, frame_count: int) -> rtc.VideoFrame:
    """
    Create a test pattern video frame (optimized for performance).
    
    Uses vectorized numpy operations for much faster frame generation.
    """
    import numpy as np
    
    # Use vectorized operations instead of loops - MUCH faster
    # Create coordinate grids
    x = np.arange(width, dtype=np.float32)
    y = np.arange(height, dtype=np.float32)
    X, Y = np.meshgrid(x, y)
    
    # Create animated gradient pattern using vectorized operations
    # This is 100x faster than pixel-by-pixel loops
    phase = frame_count * 0.1
    r = (128 + 127 * np.sin((X + phase) * 0.02)).astype(np.uint8)
    g = (128 + 127 * np.sin((Y + phase) * 0.02)).astype(np.uint8)
    b = (128 + 127 * np.sin((X + Y + phase) * 0.02)).astype(np.uint8)
    
    # Stack into RGB frame
    frame_data = np.stack([r, g, b], axis=2)
    
    # Add simple centered circle (more efficient than rectangle)
    center_x, center_y = width // 2, height // 2
    radius = min(width, height) // 8
    circle_mask = ((X - center_x)**2 + (Y - center_y)**2) < radius**2
    frame_data[circle_mask] = [255, 255, 255]
    
    # Convert RGB to RGBA (add alpha channel)
    rgba_frame = np.zeros((height, width, 4), dtype=np.uint8)
    rgba_frame[:, :, 0] = frame_data[:, :, 0]  # R
    rgba_frame[:, :, 1] = frame_data[:, :, 1]  # G
    rgba_frame[:, :, 2] = frame_data[:, :, 2]  # B
    rgba_frame[:, :, 3] = 255  # A
    
    # Create VideoFrame from buffer
    buffer = rgba_frame.tobytes()
    video_frame = rtc.VideoFrame(
        width=width,
        height=height,
        type=rtc.VideoBufferType.RGBA,
        data=buffer,
    )
    
    return video_frame


async def _generate_tavus_frames(
    video_source: rtc.VideoSource,
    width: int,
    height: int,
    fps: int,
):
    """
    Generate video frames using Tavus API.
    
    This is a placeholder - you would integrate with Tavus API here.
    Tavus typically provides:
    - Real-time avatar rendering API
    - WebSocket streams for video frames
    - Audio-synchronized video output
    """
    logger.info("Tavus integration - placeholder")
    logger.info("To implement:")
    logger.info("1. Set up Tavus API client with your API key")
    logger.info("2. Create a replica or use existing replica ID")
    logger.info("3. Stream video frames from Tavus API")
    logger.info("4. Capture frames and send to video_source.capture_frame()")
    
    # For now, use placeholder frames
    await _generate_placeholder_frames(video_source, width, height, fps)
    
    # Example structure for actual implementation:
    # tavus_api_key = os.getenv("TAVUS_API_KEY")
    # replica_id = os.getenv("TAVUS_REPLICA_ID")
    # 
    # # Connect to Tavus streaming API
    # async with tavus_client.stream_video(replica_id) as stream:
    #     async for frame_data in stream:
    #         # Decode frame data to VideoFrame
    #         video_frame = decode_tavus_frame(frame_data, width, height)
    #         video_source.capture_frame(video_frame)  # Note: capture_frame is synchronous


async def _generate_beyond_presence_frames(
    video_source: rtc.VideoSource,
    width: int,
    height: int,
    fps: int,
):
    """
    Generate video frames using Beyond Presence API.
    
    This is a placeholder - you would integrate with Beyond Presence API here.
    """
    logger.info("Beyond Presence integration - placeholder")
    logger.info("To implement:")
    logger.info("1. Set up Beyond Presence API client")
    logger.info("2. Configure avatar ID")
    logger.info("3. Stream video frames synchronized with audio")
    logger.info("4. Capture frames and send to video_source.capture_frame()")
    
    # For now, use placeholder frames
    await _generate_placeholder_frames(video_source, width, height, fps)


async def sync_video_with_audio(
    video_source: rtc.VideoSource,
    audio_track: rtc.AudioTrack,
    provider: str = "tavus",
):
    """
    Synchronize video frame generation with audio output.
    
    This ensures the avatar's lip movements match the audio being played.
    """
    logger.info(f"Syncing video with audio for {provider}")
    # Implementation would:
    # 1. Monitor audio track for speech
    # 2. Generate corresponding video frames
    # 3. Ensure frame timing matches audio playback
    pass
